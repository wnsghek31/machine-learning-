==# 1.

## 지도학습
비지도학습은 거의 사용하지않는다. 잘되는것은 거의 지도학습이다. 
인공지능회사들 레이블링하는데 사람을 많이쓴다.



## 텐서플로우
수학공식을 그래프로 바꿔서   (Data Flow Graph)
노드는 수학적 연산을 나타내며 , 

tensorflow는
1. build graph using TensorFlow operations (node로?)
2. feed data and run graph (operation , sess.run)
3. update variable in the graph

deep learning에서는 float32 float64 같이 16bit 32bit 64bit의 차이가 중요함
(16bit로 하면 더 빠르니까 , 데이터가많으면 느려서 속도 높이기위해)

reduce_mean : sum 해서 평균

**텐서플로우 워크플로우**

1. 변수와 hypothesis 만들고
2. cost함수 만들고
3. optimizer 를 만들고
4. sess.run 돌리면서 반복

그래픽연산떄문에 sess.tf.Session   sess.run(adding) 으로 코딩해줘야하는것
이 코드들 전까지만 치는것은 그래프 구조를 만들어주는 작업을한것이고 이것으로 그래프를 실행시키는것이다.

`/content/gdrive/My Drive/ `까지는 똑같고   거기서 내가만든 Colab Notebooks에있는 내파일

```
from google.colab import drive
drive.mount('/content/gdrive')

df = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/DeepLearningZeroToAllExample/Admission_Predict_Ver1.1.csv')
# 구글 드라이브 mount 해주니까 pandas로도 읽어올수잇다.
x = df.iloc[:,1:-1].values
# dataframe에서는 slice를 iloc으로 한다  , [:,:] [행,열]
y = df.iloc[:,[-1]].values
# 밑에다가 sess.run 해서 넣을떄 numpy array 를 넣어야대는데   . .values()하면 array로 바뀐다.
```

placeholder로 선언해주면 아주 flexible하게 만들수있따. 어떤 형태이든 들어갈수있어

we use `tf.Variable` for trainable variables such as weights (W) and biases (B) for your model.
`tf.placeholder` is used to feed actual training examples.

`W = tf.Variable(tf.random_normal([3, 1]), name='weight')`
W의 행의 수는 피쳐의 개수




## 비지도학습
구글의 뉴스 그룹핑
클러스터링



## 뉴럴네트워크

현실적으로 local minimum을 찾는다 . global minimum을 찾을 방법은없다..

이 간단한 gradient descent는 간단할때에만 가능하고 , 
local minimum을 건너뛰고싶은 경우가 있끼때문에 , 더 좋은 방법을 쓴다



출력이 n개인 경우  (출력이 n개인 경우가 현실세계에선 많이있따)
> 게임플레이를 할때 좌표는 x,y로 나타내야하니까


비선형함수인데 제곱을 취하면 로컬미니멈이 엄청나게생김

layer가 한개면 한계가있따. 태생적으로 못푸는문제들이.	


크로스엔트로피에서 -log 를 곱하니까 ,  log 형태의 convex한 cost function이 나오게되서 gradient descent 된다.

크로스엔트로피 ,  소프트맥스 다 exponenetial이 있어서 log를 취했을때 convex

분류문제에 있어서는 sigmoid 나 tanh를 취해주니까 비선형 함수가 된다. 그래서 제곱을 취했을때 로컬미니멈 많이생기는 울퉁불퉁한 그래프가...
(분류문제 풀라고 시그모이드 취했떠니 비선형이라(?) 제곱하니까 울퉁불퉁하네?? 펴줘야겟다 -> log)

sigmoid tanh  같은것이 다 비선형인데 .. costfunction을 계싼하기위해서 log(?)를 취해서(==log를 취하는게 아닌 다른방법도?==)



데이터 전처리

1. 중심점을 0 가까이로 갖다놓는것.
2. ++표준화(standardization, = normalization 정규분포로 만드는것  )(너무 멀리간 이상값들을 처리할수있따. 영역안으로 넣어서 엄청 큰차이가안나게하는듯?)++
3. ++스케일링 (min-max scale)++

++는 많이 쓰이는것.

데이터가 많은 경우 validation 까지 ,  파라미터 튜닝용으로 . 데이터많으면 학습한번에 몇일씪 걸리니까....


### softmax

Softmax function은 Neural network의 최상위층에 사용하여 Classification을 위한 function으로 사용한다. NN의 최상위 계층에 사실 Sigmoid function을 사용하여 실제값과 출력값을 차이를 error function으로 사용해도 된다. 하지만 Softmax function을 사용하는 주요한 이유는 결과를 확률값으로 해석하기 위함( 다른 출력값들과 상대적 비교과 된다. 클래스들 확률 합은 1이 되기에 normalization 효과까지)

※. Softmax function은 logistic regression이 2가지 레이블 분류하는 것과 달리 여러개의 label에 대해서도 분류(multiclas classification)를 가능하게 한다. 2개의 레이블에 대한 분류에 대해 Softmax function과 logistic function은 같다(동치). 


### Activation Function

Activation function은 말 글대로 활성함수로 각 뉴런에 입력된 데이터에 대한 뉴런의 출력이다.
즉 Neural Network에서는 한 노드는 inputs과 각 weights과의 곱을 모두 더한 값을 입력는데, 받아들인 이 값이 Activation function을 지난 값이 이 뉴런이 출력한 output

**미분이 가능해야 gradient descent를 쓰기때문에 미분이 가능한 activation function을 써야댐**


## 깊이가 깊어질때의 문제


잘못된 비선형 함수를 사용해서  결정적으로 성능이 안나왔떤것이다. 

### Vanishing Gradient Problem

backpropagation이 뒤로가면서 학습시키는데 , 뒤쪽으로 갈수록 영향을 못미쳤따. (5~6년전 해결)

XOR문제인데도 9 depth 까지가면 0.5의 정확도가나온다 2depth만해도 1 나왔엇는데,



#### ReLU ( Max(0,x) )
sigmoid function에서의 Gradient Vanishing문제 떄문에 해결법으로써 쓰이기시작함. 
sigmoid fuction으로 인해 0~1의 값을 가지는데, gradient descent를 사용해 backpropagation 수행시 layer를 지나면서 gradient를 계속 곱하므로 gradient는 0으로 수렴하기에 layer가 많아지면 잘작동하지않았떤것

**hidden layer에는 ReLU를 쓰지만 맨 마지막은 sigmoid를 써서 0~1사이 값을 받아야함**

#### 이점
1. Sparse activation : 0 이하의 입력에 대해 0을 출력함으로 부분적으로 활성화 시킬수있따
2. Efficient gradient propagation : gradient의 vanishing이 없으며 gradient가 exploding 되지 않는다
3. Efficient computation : 선형함수이므로 미분 계싼이 매우 간단하다

==시그모이드함수에서의 경사값이아니라, 시그모이드함수에서나온 결과에 코스트함수에서의 경사
그러니까 ReuLU를 써서 정확도가 올라가지 . 이건 음수는 0으로 만들어버리니까  (먼소리지)==


### 초기값설정

초기값이 러닝에 많은 영향을 끼치더라..

초기값을 정하기위해 Restricted Boatman Machine 썼었는데 ,너무 복잡하고 오래걸려서 
`( 입력 개수 , 출력의 개수 중 난수 하나를 뽑아 ) / (입력 개수)의 제곱근?` 으로 Weight을 주니까 학습이 엄청 잘되더라

`( 입력 개수 , 출력의 개수 중 난수 하나를 뽑아 ) / (입력 개수 / 2)의 제곱근?` 으로 하니까 훨씬 더 잘되더라..

### DropOut
오버피팅 방지를위해 Drop out
학습을 진행할때 랜덤으로 몇개에 노드는 학습 시키지않는다 (500개 학습하고 500개는 쉬고 )
이걸 반복해가면서..  [들어오는 데이터에 대해서 어떤 노드는 학습되고 어떤애는 안되고..]
**드롭아웃 비율은 0.5를 많이 준다.**
왜이렇게 되는지에대한 가설(?)의 예로 사진의 종류에따라서 고양이 귀가 가려졌따고 고양이가 아닌게 아닌데 (눈이 두개있어야지 고양이라고하는 모델은 정확한 모델이 아닌것) , 
**dropout은 분류문제에서 많이쓰이지만 회귀문제에선 거의 안쓰여... 몇개뺴고학습하는게 회귀에선 안좋은듯**

#### dropout의 효과
#####  voting 효과
mini-batch 로 줄어든 망으로 학습하게되면 그망은 그망 나름대로 overfitting되고 , 다른 mini-batch 망도 일정정도 overfitting된다 . 이를 무작위 반복하면 voting에의해 평균효과를 얻을수있따.

##### parameter들의  co-adaptation을 피하는 효과
특정 뉴런의 바이어스나 가중치가 큰값을 갖으면 그것의 영향이 커지면서 다른 뉴런들의 학습속도가 느려지거나 학습이 제대로 안됨 하지만 drop out하면 어떤 뉴런의 가중치나 바이어스가 특정 뉴런의 영향을 받지 않기 떄문에 (랜덤하게 선택되기에 어느것은 그 특정 뉴런이 꺼져있으면 영향안받고 어떤건 받게되기에?)굿. 
특정 학습 데이터나 자료에 영향을 받지 않는 보다 robust한 망을 구성

뉴런들을 무작위로 생략시키면서 학습시키면 parameter들이 서로 동화(co-adaptation) 되는것을 막을수있어, 좀더 의미 있는 특징들을 더 추출한다. 즉 다른 파라미터와 같이 cost function을 줄여나가다 보면 파라미터의 공조현상이 일어날수있는데 , Dropout을 하게 되면서 서로 의지하던것을 스스로 해줘야하기때문에 좀더 의미있는 feature를 끄집어낸다

파라미터 0.4 이하에선 underfitting , 0.8 이상에선 overfitting이 되는..
뉴런개수 많으면 , 비율이 0.3 0.4 해도 뉴런개수 적을떄보다 underfitting 피한다 (당연)
0.5가 효과적이다.

DropConnect란 방법도 있는데 거의 비슷한 성능을



depth가 152개가되고 너무 deep해지니까 학습이 잘안되니까 하기위해서 fast foward . weight을 앞으로보내서 더해줌 


