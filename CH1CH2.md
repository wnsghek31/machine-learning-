# 파이썬 라이브러리를 활용한 머신러닝

머신러닝은 데이터에서 지식을 추출하는 작업
사용할 데이터를 이해하고 그 데이터가 해결해야할 문제와 어떤 관련이 있는지를 이해해야한다.

## Chapter 1

### 지도학습
레이블이 붙은 훈련용 데이터로 지도학습 알고리즘을 훈련시키는데 
이 뜻은 원하는 답으로 구성된 데이터로 알고리즘을 훈련시킨다는 의미이다.
입력 데이터로부터 기대한 출력이 나오도록 알고리즘을 가르치는것이기 때문에 입력과 출력으로부터 
학습하는 머신러닝 알고리즘을 지도 학습이라고한다.
(회귀와 분류)

### 비지도학습
특성이 비슷한 데이터를 합쳐서 군(Group)으로 분류하는 학습방법이다.
(흔히 알고있는 군집화(Clustering) 기법이  일종, 주어진 입력과 X와 비슷한 입력들의 군집을 추정해내는것)
결과정보가 없는 데이터들(레이블이 없는) 에 대해 특정 패턴 (feature) 을 찾는 것이다.
즉 사람의 개입없이 컴퓨터 스스로 데이터를 훈련해서 데이터안에서 어던 관계(Relationship)를 찾아내는것

#### 분류와 군집화
'분류' 란,   주어진 데이터 집합을 이미 정의된 몇 개의 클래스로 구분하는 문제
(즉 Label 이 있는 data를 정해진 클래스로 구분하는 것으로 지도학습의 일종)
'군집화' 란, 입력 데이터의 분포 특성(입력값의 유사성)을 분석하여 임의의 복수 개의 그룹으로 나누는 것
(즉 Label이 없는 data를 어떻게 분류하면 좋을지를 군집단위로 나누는 방법으로 비지도학습의 일종) 

군집화는 label 정보(정답) 없이 특성이 비슷한 객체 끼리 묶어보는거고 , 
분류는 label 정보가 있기 때문에 독립변수(x) 로 종속변수(y)를 예측하도록 학습을 하는것


### Numpy
고성능의 수치계산을 위해 만들어졌고 다차원 배열을 위한 기능과 
선형대수 연산과 푸리에 변환 같은 고수준 수학 함수들로 벡터및 행렬연산이 매우편리하다.
scikit-learn 은 Numpy 배열 형태의 데이터를 입력받는다.


### Scipy 
과학 기술계산용 함수 및 알고리즘 을 모아놓은 파이선 패키지
희소 행렬 기능을 제공해주는 scipy.spars 이 중요한 기능이다.

### matplotlib
과학 계산용 그래프 라이브러리. 선 그래프, 히스토그램 , 산점도 등을 지원
데이터와 분석결과를 다양한 관점에서 시각화해보면 매우 중요한 통찰을 얻을수있따.

### pandas
데이터 처리와 분석을 위한 파이썬 라이브러리
DataFrame이라는 데이터 구조를 기반으로 만들어졌다.
DataFrame은 엑셀의 스프레드 시트와 비슷한 테이블 형태라고 할수있다 
pandas는 이 테이블을 수정하고 조작하는 다양한 기능을 제공합니다.
SQL , 엑셀 , CSV 파일 같은 다양한 파일과 DB에서 데이터를 읽어 들일수 있는 것이 pandas가 제공하는 또 하나의 유용한 기능이다.

## Chapter2

### Bias , Variance

에러는 noise 와 bias , variance의 합.
관측에서 필연적으로 예측값과 참값 사이의 에러가 존재할수밖에없다. 이것을 노이즈. 

Bias란 학습데이터를 충분히 표현할 수 없기 때문에 발생. 충분한 데이터가 있지만 , 
데이터 안에 있는 데이터 간의 상관관계를 충분히 풀어내지 못할떄 발생한다.
이런 높은 Bias를 보이는 모델은 underfitting

Variance란 트레이닝 데이터에 너무 민감하게 반응하여 발생.
높은 Variance를 띄는 모델은 overfitting

![](https://github.com/wnsghek31/machine-learning-/blob/master/biasvariance.PNG)

variance는 예측한 값이 예측값의 평균을 중심으로 얼마나 퍼져있는지를 나타내주고 (예측값이 얼마나 큰 변동성을 갖고있냐), 
bias는 예측값의 평균과 실제값의 차이로 모델이 맞추지 못하는 부분을 나타낸다 (예측값이 '전반적으로' 실제값을 얼마나 정확하게 예측하냐).

![](https://github.com/wnsghek31/machine-learning-/blob/master/Inkedbiasvariance수식_LI.PNG)

이 두 개념은 서로 다르게 움직이는데, 종종 Bias를 해결하면 , Variance가 올라가고 반대도.. 즉 TradeOff 관계이다.

### Overfitting , Underfitting

학습된 모델이 처음 보는 데이터에 정확하게 예측할수있으면 이를 훈련셋 에서 데스트 셋으로 일반화(generaliztaion) 됬다고 한다. 이게 되야해.

![](https://github.com/wnsghek31/machine-learning-/blob/master/undeerfitovefit.PNG)

직선은 변동성(fluctuation)이 적어 상대적으로 분산은 작게나타나지만 , 예측값과 평균과 실제값과는 거리가 멀어져 편향은 상대적으로 큼(under)
복잡한 곡선형 모델은 예측값 평균과 실제값의 거리는 좁아져 상대적으로 편향은 낮지만 , 예측값의 변동성이 커져서 상대적으로 큰 분산 (over)

##### 과대적합
종속변수에 영향을 주는 조건이 아닌 단순한 '잡음'을 영향을 주는 조건으로 착각하고 모델에 반형한것.
가진 정보를 모두 사용해 복잡한 모델을 만드는것. 그랬을때 , 모델이 훈련데이터에 너무 잘맞지만 일반성이 떨어진다.
훈련셋의 성능이 테스트셋의 성능 보다 많이 높을 때(훈련 : 0.97 , 테스트 : 0.65)

**해결방법**
> 1. 훈련데이터를 더 많이 모음 
> 2. 정규화(Regularization)
> 3. 훈련 데이터 잡음을 줄임 (오류 수정과 이상치 제거)

##### 과소적합
데이터의 내재된 구조를 학습하지 못할만큼 너무 간단한 모델을 만드는것 (몇명 중요한 조건들을 재대로 반영하지 못함)
훈련셋과 테스트셋의 성능이 매우 비슷할 때 (훈련 : 0.953 , 테스트 : 0.958)

**해결방법**
> 1. 파라미터가 더많은 복잡한 모델을 선택
> 2. 조기종료 시점(overfitting 되기 전에)까지 충분히 학습

모델의 복잡도는 훈련 셋에 담긴 데이터의 다양성과 관련이 깊은데, 데이터 셋에 다양한 데이터 포인트가 많을수록
과대적합 없이 더 복잡한 모델을 만들수있다. 보통 데이터 포인트를 더 많이 모으는것이 다양성을 키워준다.
그러나 중복되거나 매우 비슷한 데이터를 모으는것은 별 도움이 되지 않는다.
(more data beats a better algorithm)

##### Regularization (정규화,일반화)
Feature가 너무 많아도 문제가 생긴다. Hypothesis function이 복잡해지기에 이 복잡한 함수는 training set의 데이터 분포를 
거의 똑같이 모델링할것이다. (overfitting) 즉 정규화는 이 모델이 너무 복잡해지지 않게 제약을 가하는 방법이다.

[정규하] == costfunction을 2차원에서 4차원으로 줄인다고해서 달라지는게있을까?? cost function은  오차의합인데?? costfunction의 차수가낮은것이 overfitting을 막는방법이 되나?? ==

가중치 (w) 의 절대값을 가능한한 작게 만드는것 즉, 특성들이 출력에 주는 영향을 최소한으로 만드는것
이것은 "local noise"가 학습에 큰 영향을 끼치지 않는다는 것을 의미하며, outlier(특이점) 의 영향을 적게 받도록 하고싶은것이다.
**모든 feautre를 유지하되 w의 크기를 작게 유지하기에 , 많은 feature가 y를 예측하는데 조금식 기여할경우 유용.**

**L1** 
![](https://github.com/wnsghek31/machine-learning-/blob/master/L1.PNG)

**L2** 
![](https://github.com/wnsghek31/machine-learning-/blob/master/L2.PNG)

L1 regularization은 통상적으로 상수 값을 빼주도록 되어있기 때문에 작은 가중치들은 거의 0으로 수렴,정말 0 이되기도
몇개의 중요한 가중치들만 남기에 , 몇개의 의미있는 값을 끄집어내고 싶은 경우에 효과적. ( sparse model에 적합)
기본 수식에서 보면 미분이 불가능한 점이기 때문에 gradient-based learning 에 적용시 주의!!
L2는 0에 가깝게 만들지만 완전히 0이 되지 않는다.


특히 딥러닝 같은 복잡한 모델은 데이터에서 미묘한 패턴을 감지할수 있어서 훈련데이터셋에 잡음이 많거나
데이터가 너무 적으면 잡음이 섞인 패턴을 감지하여 학습할수 있기 때문에 더 주의해야함.

### k-NN 알고리즘

훈련 데이터 셋을 그냥 저장하는 것이 모델을 만드는 과정의 전부

![](https://github.com/wnsghek31/machine-learning-/blob/master/knn.PNG)

새로운 데이터 포인트에 대해 예측할 땐 훈련 데이터 셋에서 가장 가까운 데이터 포인트 즉 '최근접 이웃'을 찾는다.
가까운 이웃 하나가 아니라 임의의 k개의 이웃들로 데이터 포인트의 레이블을 결정 할수있기에 , k-NN
k=1이면 젤 가까운애의 클래스 , k>=2 이면 근접한 데이터 포인트들에 레이블을 보고, 제일 많은 클래스.
회귀에 사용될 때는 여러개의 이웃들의 값의 평균이 예측이 된다.

### 선형모델

입력특성들에 대한 선형 함수를 만들어 예측을 하는것.
예측을 만들기위해 feature 들의 단일 가중치 합을 사용한다.
선형 모델은 특성이 하나일땐 직선 , 두개 일 땐 평면 , 더 높은 차원(특성이 많음)에서는 초평면이 된다.

#### 회귀용 선형모델 

##### 선형회귀 
`y = w[0] * x[0] + w[1] * x[1] ... + w[p] * x[p] + b`

![](https://github.com/wnsghek31/machine-learning-/blob/master/Inked선형그림_LI.PNG)

supervised learning의 아주 간단한 방법인 선형회귀.
다른 통계방법에 비해 간단하나 해석력이 뛰어나 여전히 널리쓰이고 있고 다른 방법들의 기초가 되는 지식이다

예측과 훈련 세트에 있는 타깃 y 사이의 **평균제곱오차 (mean squared error)** 를 최소화 하는 파라미터 w 와 b를 찾는것
이 mean squared error가 신경망의 cost function으로 흔히 사용된다
MSE = 오차의 제곱의 합

선형이란것이 1차원을 말하는것이 아니다.

![](https://github.com/wnsghek31/machine-learning-/blob/master/선현선형.PNG)

그림 둘다 선형모델이다. 
선형 회귀 모델은 '회귀 계수'를 선형 결합으로 표현할수 있는 모델 이라는 말이다.

![](https://github.com/wnsghek31/machine-learning-/blob/master/선현선형선형2.PNG)

예를들어 이런 식들도 독립변수 x를 기준으로 생각하면 x^2 , x^3 때문에 비선형이라고 생각 할수있는데, 
회귀 모델의 선형성은 x가 아니라 회귀 계수인 B0 , B1 , B3을 기준으로 하는것이기에 선형 회귀식이다.

L2 -> 릿지
L1 -> 라쏘

##### 비선형 회귀 모델이란 ?

![](https://github.com/wnsghek31/machine-learning-/blob/master/비선형.PNG)

비선형 모델은 데이터를 어떻게 변형하더라도 파라미터를 선형 결합식으로 표현할수없는 모델.
선형 회귀 모델은 해석이 단순하지만 , 비선형 모델은 형태가 복잡할 경우 해석이 매우 어렵다. 
그래서 모델의 해석을 중시하는 통계 모델링에서는 비선형 회귀 모델은 잘 쓰지 않는다.
하지만 매우 유연하기 때문에 복잡한 패턴을 갖는 데이터에 대해서도 모델링이 가능하다.
그래서 많은 데이터를 갖고있어서 variance error를 충분히 줄일수 있다면 많이 쓰인다. 
대표적으로 딥러닝이라고 불리는 뉴럴 네트워크에서도 많이 쓰인다.


#### 분류용 선형모델
`y = w[0] * x[0] + w[1] * x[1] ... + w[p] * x[p] + b > 0 `

특성들의 가중치 합을 그냥 사용하는 대신 예측한 값을 임계치 0과 비교해서 0보다 크고 작고로 클래스를 나눈다

![](https://github.com/wnsghek31/machine-learning-/blob/master/분류용선형모델.PNG)


##### 로지스틱회귀 (logistic regression)
이는 독립 변수의 선형 결합으로 종속 변수를 설명한다는 관점에서는 선형 회귀 분석과 유사함
Regression이란 말이 들어가지만 회귀 알고리즘이 아니라 분류 알고리즘이다. LinearRegression과 다른것이여
흔히 로지스틱 회귀는 종속변수가 이항형 문제(유효한 범주가 두개)를 지칭할 때 사용된다. 범주가 두개이상이면 다항 로지스틱 회귀

[종속 변수가 이진적이기 때문에 조건부 확률(P(y│x))의 분포가 정규분포 대신 이항 분포를 따른다는 점이다 ??]

이진 분류에선 값이 0,1 이 되야하기에 값을 0 ~ 1로 만들어줘야하는데, linearRegression 으로 값을 예측하면 예측값이 -Inf ~ Inf의 범위를 갖는다.
그래서 y = w[0] * x[0] + w[1] * x[1] ... + w[p] * x[p] + b 를 간단하게 0 ~ 1로 압축시켜주는 함수가 하나 있으면 좋겠다고 생각!
x값이 -Inf ~ Inf 이지만 y값이 0~1이 되게하는 이 함수를 시그모이드함수라고한다.

![](https://github.com/wnsghek31/machine-learning-/blob/master/시그모.PNG)

z = w[0] * x[0] + w[1] * x[1] ... + w[p] * x[p] + b 로 하고 z값을 이 함수에다 넣음 -> 0~1값 -> 이값으로 분류

##### 다중 클래스 분류용 선형 모델
로지스틱 회귀만을 제외하고 많은 선형 분류 모델은 태생적으로 이진 분류만을 지원한다. (linearSVC)
이진 분류 알고리즘을 다중 클래스 분류 알고리즘으로 확장하는 보편적인 기법은 일대다(one vs rest) 방법이다.

onv vs rest 방식은 각 클래스를 자신의클래스(1) or 다른 모든 클래스(0)와 구분하도록 이진 분류 모델을 학습한다.
결국 클래스의 수만큼 이진 분류 모델이 만들어진다. 그러면 예측할때는 모든 이진 분류기가 작동하여 가장 높은 점수를
내는 분류기의 클래스를 예측값으로 선택한다.

![](https://github.com/wnsghek31/machine-learning-/blob/master/일대다.PNG)

중앙의 삼각형은 모두 나머지로 분류한 영역인데 , 이곳은 가장 가까운 직선의 클래스가 된다.

one vs one 방식은 K 개의 타겟 클래스가 존재하는 경우, 이 중 2개의 클래스 조합을 선택하여  K(K−1)/2 개의 이진 클래스 분류 문제를 풀고 이진판별을 통해 가장 많은 판별값을 얻은 클래스를 선택하는 방법
클래스의 수가 많아지면 실행해야 할 이진 판별 문제의 수가 너무 많아져서 one vs rest를 쓴다.

다중 클래스 로지스틱 회귀 이면의 수학은 일대다 방식과 조금 다르지만, 여기서도 클래스마다 하나의 계수벡터와  절편을 만들며 , 예측 방법도 같다.


### 나이브 베이즈 분류기
허어

### 결정트리

결정트리는 결정에 다다르기 위해 예/아니오 질문을 이어나가면서 학습한다.

![](https://github.com/wnsghek31/machine-learning-/blob/master/결정트리1.PNG)

위 사진은 결정트리의 모습인데, 
세개의 특성 "날개가 있나요?" , "날수있나요?" , "지느러미가 잇나요?" 를 사용해
네개의 클래스(매,펭귄,돌고래,곰)를 구분하는 모델인것이다.
이런 모델(트리)을 직접 만드는것이 아닌 지도 학습 방식으로 데이터로부터 학습해서 만들수 있다.
트리를 만들때는 알고리즘은 가능한 모든 테스트에서 타깃값에 대해 가장 많은 정보를 가진 것을 고른다.

![](https://github.com/wnsghek31/machine-learning-/blob/master/결정트리2.PNG)

위에 그림을 보면 x[1] =0.0596에서 수평으로 나누는것이 가장 많은 정보를 포함한다. 즉 이 직선이 클래스 0과 1을 가장 잘나누고있다. (이런 나누는 짓을 현재 노드(상황)에서 최선의 테스트(분류기준?)를 찾는다.) 
이 두영역에서 가장 좋은 테스트를 찾는 과정을 반복해서 모델을 정확하게 만들어나간다.
계층적으로 영역을 분할해가면서 각 분할된 영역이 (결정 트리의 리프) 한개의 타깃값(하나의 클래스 or 하나의 회귀값)을 가질 때 까지 반복한다.
분류에서는 새로운 데이터 포인트에 대한 예측은 그 영역의 타깃값중 다수(순수 노드라면 하나) 인 것을 예측 결과로 합니다.
회귀에서는 찾은 리프노드의 훈련 데이터 평균값이 이 데이터 포인트의 출력이다.

복잡도 제어하기
순수 노드로 이루어진 트리는 훈련 세트에 100% 정확하게 맞는다는 의미이다. (overfitting)
그래서 전략으로 트리 생성을 일찍 중단하는 전략(사전 가지치기 pre-pruning) , 트리를 만든후 데이터 포인트가 적은 노드를 삭제하거나 병합하는 전략(가지치기 pruning)이 있다.

주의점
티로메들은 가진 데이터 범위 밖으로 나가면 단순히 마지막 포인트를 이용해 예측하는게 전부이다.
트리모델은 훈련데이터 범위 밖의 새로운 데이터를 예측할 능력이 없습니다. 이는 모든 트리 기반 모델의 공통된 단점

#### Ensemble
동일한 학습 알고리즘을 사용해서 여러 모델을 학습하는 개념 Weak learner들을 결합한다면, Single learner보다 더 나은 성능을 얻을 수 있다는 아이디어다
(weak learne = only just above chance performance).
랜덤포레스트와 그레디언트 부스팅 알고리즘이있다.

#### 랜덤포레스트
결정트리의 주요 단점인 훈련데이터에 과대적합되는 문제를 회피하기위해 나온 방법으로 조금씩 다른 여러 결정 트리의 묶음이다.
잘작동하되 서로 다른 방향으로 과대적합된 트리를 많이만들면 그 결과를 평균냄으로써 과대적합을 줄일수있다 (수학적으로 증명됨)
각 트리들은 타깃 예측을 잘해야하고 , 다른 트리와는 구별되야하기에 트리들이 달라지도록 트리 생성시 무작위성을 주입해야함
랜덤 포레스트에서 트리를 랜덤하게 만드는 방법은 트리를 만들때 사용하는 데이터 포인트를 무작위로 선택하는 방법과 분할 테스트에서 특성을 무작위로 선택하는 방법이다.

트리르 만들기 위해서는 먼저 데이터의 부트스트랩 샘플을 생성한다. 이 것은 n개의 데이터 포인트중에서 무작위로 n횟수만큼 반복추출한다는 것이다.
(한샘플이 여러번 중복추출될수있음) 이렇게해서 만들어지는 데이터셋은 원래 데이터셋과 크기는 같지만 어떤 데이터 포인트는 누락될수도있고(대략 1/3) 어떤 데이터포인트는 중복으로 들어갈수도있다.

[a,b,c,d,] 로 부트스트랩 샘플을 만든다면 [b,d,d,c] , [d,a,d,a] 등이 나올수있는것

이렇게 만든 부트스트랩 샘플로 트리를 만드는데 , 각 노드에서 전체 특성을 대상으로 최선의 테스트를 찾는것이 아닌, 
각 노드에서 후보특성을 무작위로 선택한 후 이 후보들 주에서 최선의 테스트를 찾는다.

[a,a,b,e,d,f,g,h] 에서 (b,e,d) 를 후보로 뽑아 이 중에서 최선을 찾는다.

이렇게 부트샘플링으로 랜덤포레스트의 트리들이 조금씩 다른 데이터셋을 이용하게 하고 , 각노드에서 특성의 일부만 사용하기에 트리의 각 분기는 각기 다른 특성 부분집합을 사용한다.
이렇게 만들어진 무작위성은 알고리즘이 가능성이 있는 많은 경우를 고려 할수 있또록 하므로, 단일 트리보다 더 넓은 시각으로 데이터를 바라볼수 있다.

#### 그레디언트 부스팅 회귀 트리
이전 트리의 오차를 보완하는 방식으로 순차적으로 트리를 만든다.
무작위성은 없지만 강력한 사전 가지치기가 사용된다. 
얕은 트리 (weak learner)들을 많이 연결하는것이 기본 아이디어

[그래디언트 디센트 알고리즘과 비슷한건가??]

#### Model Stacking
동일한 학습 알고리즘을 사용하는 방법을 앙상블이라고 한다면, 서로 다른 모델을 결합하여 새로운 모델을 만들어내는 방법도 있습니다. 대표적으로 Model Stacking 이 있다. 
캐글 데이터 분석 경진대회의 수상자를 살펴보면 대부분의 수상자들은 최신 머신러닝 알고리즘(XGB, GBM, RF, DL)을 결합한 모형 쌓기 방법(stacking)을 사용했다.

다양한 알고리즘을 사용하여 기초모형을 개발하고 기초모형의 예측값을 입력값으로 메타모형을 만들어서, 보다 예측력이 높은 모형을 만드는 것

![](https://github.com/wnsghek31/machine-learning-/blob/master/modelstacking.PNG)

모형 쌓기에서 학습데이터를 생성하는 모습이다. 모형1부터 5까지 예측값(Predict)을 생성하고 예측값을 모아서 새로운 학습데이터(New Feature)을 생성한다. 마지막으로 새롭게 생성된 학습데이터를 입력값으로 Model6를 개발한다

![](https://github.com/wnsghek31/machine-learning-/blob/master/모델스테킹.PNG)

Bob,Kate,Mark,Sue를 분류하는 문제에서  KNN과 SVM을 사용했따.
KNN은 Mark와 Kate를 잘분류하고 , SVM은 BOb과 Sue를 잘 분리하는 장점을 가지고 있끼에 둘을 결합하여 높은 정확도의 모델을 만들었다.


장단점
다양한 알고리즘의 장점을 취합할 수 있다. 알고리즘별로 학습하는 방식이 서로 다르기 때문에 서로 결합 됬을 경우에 예측력이 높아질 가능성이 높다.
하지만 기초모형 개발에 노력과 시간이 많이 소요되고 , 단일 모형만으로도 충분히 예측력이 높을경우에는 결합을 통한 향상 효과가 미미할수도.

주의할점은 model stacking을 할때 만약 모델 A,B를 개발해서 stacking을 한다면 두 모델의 상관관계가 높다면 얻을수 있는 효과가 높지않을수도있다. 
또한 단일 모형의 예측 선응이 다른 모형의 예측 성능보다 월등히 높은 경우에는 모형을 결합하면 오히려 예측성능이 떨어질수있다.

#### 서포트 벡터 머신
일반적으로 훈련 데이터의 일부만 결정 경계를 만드는데 영향을 준다. 
바로 두 클래스 사이의 경계에 위치한 데이터 포인트들인데 이런 데이터 포인트를 서포트 벡터라고한다.

![](https://github.com/wnsghek31/machine-learning-/blob/master/서서포트.PNG)

B1이 더 좋은 분류 경계면이라고 할수있따. 두 범주를 여유있게 가르기 때문에.
위그림에서 b12를 minus-plane , b11을 plus-plane이라하고 이둘 사이의 거리를 마진이라고 하는데 . SVM은 이 마진을 최대화 하는 분류 경계면을 찾는 기법이다. ( 이 plane들 위에 있는 점이 서포트 벡터 )


#### 커널 서포트 벡터 머신
입력데이터에서 단순한 초평면(hyperplane)으로 정의 되지 않는 더 복잡한 모델을 만들수 있또록 확장한 것.
직선과 초평면은 유연하지 못하여 저차원 데이터셋에서는 선형 모델이 매우 제한적이다.
그래서 선형 모델을 유연하게 만드는 한 가지 방법은 특성끼리 곱하거나 특성을 거듭제곱하는 식으로 새로운 특성을 추가하는것

![](https://github.com/wnsghek31/machine-learning-/blob/master/svm1.PNG)

선형 모델은 직선으로만 데이터 포인트를 나눌 수 있어서 , 이렇게 잘 안나눠질수있는데, (특성1)^2를 새로운 특성으로 추가해 입력특성을 확장한다면 
원래는 (특성0,특성1)인 2차원 데이터 포인트였지만 , (특성0,특성1,(특성1)^2) 의 3차원 데이터 포인트로 표현된다.

![](https://github.com/wnsghek31/machine-learning-/blob/master/svm2.PNG)

3차원의 평면을 사용해서 두클래스를 나눌수있다.

![](https://github.com/wnsghek31/machine-learning-/blob/master/svm3.PNG)

원래 특성에 다시 투영하면이 선형 SVM 모델은 더이상 선형(??)이 아니다.   [선형이라는 의미가 선형결합이안된다는?]

그런데 고차원으로 매핑하면 연산량이 폭증하게되기에 수학적 기교를 사용해서 새로운 특성을 많이 만들지 않고서도 고차원에서 분류기를 학습 시킬 수 있다.
실제로 데이터를 확장하지 않고 확장된 특성에 대한 데이터 포인트들의 거리를 계산하는 것을 커널기법이라한다.


#### 신경망

##### 퍼셉트론
퍼셉트론은 다수의 신호(Input)을 받아서 하나의 신호(Output)을 출력한다.
선형분류를 수행 할수있는 피드 포워드 뉴럴 네트워크.

##### 다층 퍼셉트론 (multiplayer percetprons)
입력층과 출력층 사이에 하나 이상의 중간층이 존재하는 신경망
이 중간층을 은닉층이라고한다.



data point : 아이리스 데이터에 잎넓이, 잎두께 같은것들
chance performance : what you would obtain if you performed at random


데이터를 충분히 주면 규제항은 덜 중요해져서 릿지 회귀와 선형 회귀의 성느이 같아질것이라는것. 왜일까??
왜 L1, L2를 붙히면 overfitting이 안되는가. 수식적 원리?
LogisticRegression은 이진 분류에서 logistic 손실 함수를 사용하고 , 다중분류에서는 교차 엔트로피 손실함수를 사용한다 .  교차엔트로피??

선형모델은 샘플에 비해 특성이 많을 때 잘 작동한다. 다른 모델로 학습하기 어려운 매우 큰 데이터셋에서도 선형모델을 많이 사용한다.
그러나 저차원의 데이터셋에서는 다른 모델들의 일반화 성능이 더 좋다   
불확실성과 모델의 정확도가 동등하면 이 모델이 보정(calibration) 되었다고 한다. 즉 보정된 모델에서 70%확신을 가진 예측은 70% 정확도를 낼것이다.


